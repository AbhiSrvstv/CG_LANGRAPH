{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U langgraph langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK78WuXaohJ8",
        "outputId": "3847c0e6-bc0f-4fde-b65e-35d013f30cfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.45)\n",
            "Collecting langsmith\n",
            "  Downloading langsmith-0.4.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.65)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.8 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TFEuFD0EoWGv"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "#AIzaSyCXp230h__4BovmnNDxBWwjIfbyZTCvUKA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##azure ai\n"
      ],
      "metadata": {
        "id": "AykFOR0Hpwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAQ0FVjupyJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J8sV3KbLoWGy",
        "outputId": "f76a652f-0c43-415c-e0b4-b6cbfe8a9b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[openai] in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[openai]) (6.0.2)\n",
            "Collecting langchain-openai (from langchain[openai])\n",
            "  Downloading langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[openai]) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[openai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[openai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[openai]) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[openai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[openai]) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[openai]) (3.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai->langchain[openai]) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai->langchain[openai]) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[openai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain[openai]) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai->langchain[openai]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai->langchain[openai]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai->langchain[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai->langchain[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai->langchain[openai]) (2024.11.6)\n",
            "Downloading langchain_openai-0.3.24-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.24\n"
          ]
        }
      ],
      "source": [
        "pip install -U \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "gvAVaSmKoWG1",
        "outputId": "060d6e59-f7a1-4cb1-eb4e-44badc6a124b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'AZURE_OPENAI_DEPLOYMENT_NAME'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2100562314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m llm = init_chat_model(\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"azure_openai:gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mazure_deployment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AZURE_OPENAI_DEPLOYMENT_NAME\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'AZURE_OPENAI_DEPLOYMENT_NAME'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"1lOkjRo9bsDnbAE2v1qVagkmEj6mT111wak1spzwGQMDsqFoVT1aJQQJ99BDAC5RqLJXJ3w3AAABACOGNvwZ\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://agentic-accelerator-pod-openai-we.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-12-01-preview\"\n",
        "\n",
        "llm = init_chat_model(\n",
        "    \"azure_openai:gpt-4o\",\n",
        "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemini\n"
      ],
      "metadata": {
        "id": "HyoVgGAEpzIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U \"langchain[google-genai]\""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tm1iTktkp0j9",
        "outputId": "745cf1e6-a20f-45b1-e31a-b6612bf108f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (6.0.2)\n",
            "Collecting langchain-google-genai (from langchain[google-genai])\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[google-genai]) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[google-genai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[google-genai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain[google-genai]) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[google-genai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[google-genai]) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain[google-genai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[google-genai]) (3.2.3)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[google-genai]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[google-genai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[google-genai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain[google-genai]) (3.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[google-genai]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.6.1)\n",
            "Downloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "02d86e3f1fc8441b804b82841d4d1d51"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCXp230h__4BovmnNDxBWwjIfbyZTCvUKA\"\n",
        "\n",
        "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "dJDh5RUFp4tV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMW9pjdTp_8X",
        "outputId": "ee9cf9a5-cd88-4dc4-ee69-8524c95706fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x791ae2ed8d50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.add_edge(START, \"chatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nceoDKDvqD0B",
        "outputId": "d5d201e5-6e86-4217-eb39-2a0a82df5919"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x791ae2ed8d50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "5Lj9jQCFqHdy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXpN8UFgqLCp",
        "outputId": "78186687-581d-4863-dd7a-995c143221b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I want to learn about multi agent in langchain\n",
            "Assistant: Okay, let's dive into multi-agent systems in Langchain. This is a powerful area that allows you to build complex applications where multiple \"agents\" collaborate to solve problems.\n",
            "\n",
            "**What are Multi-Agent Systems?**\n",
            "\n",
            "In essence, a multi-agent system (MAS) is a system composed of multiple intelligent agents that interact with each other and their environment.  Each agent has its own goals, knowledge, and capabilities. The agents communicate, coordinate, and negotiate to achieve individual or collective goals.\n",
            "\n",
            "**Why Use Multi-Agent Systems in Langchain?**\n",
            "\n",
            "*   **Complex Problem Solving:** Decompose complex tasks into smaller, more manageable sub-tasks that can be handled by specialized agents.\n",
            "*   **Specialization:** Different agents can be equipped with different tools, knowledge, and expertise. This allows you to leverage the strengths of each agent.\n",
            "*   **Collaboration:** Agents can work together to achieve a common goal, sharing information and coordinating their actions.\n",
            "*   **Scalability:** Adding more agents to the system can potentially scale the system's capabilities.\n",
            "*   **Flexibility and Adaptability:** Multi-agent systems can be designed to adapt to changing environments and requirements.\n",
            "*   **Mimicking Real-World Interactions:**  Many real-world scenarios involve multiple actors with different roles and responsibilities. MAS can model these scenarios more naturally.\n",
            "\n",
            "**Key Concepts in Langchain Multi-Agent Systems**\n",
            "\n",
            "1.  **Agents:**\n",
            "    *   The core building blocks.  An agent is an entity that can perceive its environment, make decisions, and take actions.\n",
            "    *   In Langchain, an agent typically consists of:\n",
            "        *   **LLM (Language Model):**  The brain of the agent.  It uses a language model (e.g., GPT-3.5, GPT-4) to process information, reason, and generate actions.\n",
            "        *   **Tools:** The agent's capabilities.  Tools can be anything from a search engine to a calculator to a database connection.\n",
            "        *   **Agent Executor:**  Manages the execution of the agent's actions, calling the appropriate tools and feeding the results back to the LLM.\n",
            "        *   **Prompt:** Instructions given to the LLM to guide its behavior.\n",
            "    *   **Types of Agents:**\n",
            "        *   **Zero-shot react description:**  Uses only the tool's description to decide which tool to use.\n",
            "        *   **React docstore:** Accesses a document store (like Wikipedia) to answer questions.\n",
            "        *   **Self-ask with search:** Asks clarifying questions to a search engine to gather more information.\n",
            "        *   **Conversational react description:** Designed for conversational interactions, remembering past turns.\n",
            "        *   **Plan-and-execute:** First plans out the steps to take, then executes those steps.\n",
            "\n",
            "2.  **Tools:**\n",
            "    *   Functions or services that agents can use to interact with the world.\n",
            "    *   Examples:\n",
            "        *   `SerpAPIWrapper`:  For searching the internet.\n",
            "        *   `WikipediaAPIWrapper`: For accessing Wikipedia.\n",
            "        *   `Calculator`:  For performing calculations.\n",
            "        *   `PythonREPLTool`:  For executing Python code.\n",
            "        *   Custom Tools: You can define your own tools to interact with specific APIs or data sources.\n",
            "\n",
            "3.  **Agent Executor:**\n",
            "    *   The component that orchestrates the agent's actions.\n",
            "    *   It takes the agent's decisions (which tool to use and with what input), executes the tool, and passes the results back to the agent's LLM.\n",
            "    *   It manages the agent's \"thought process\" by keeping track of the agent's observations and actions.\n",
            "\n",
            "4.  **Memory:**\n",
            "    *   Agents often need to remember past interactions to make informed decisions.\n",
            "    *   Langchain provides various memory implementations, such as:\n",
            "        *   `ConversationBufferMemory`: Stores the entire conversation history.\n",
            "        *   `ConversationSummaryMemory`: Summarizes the conversation history to save space.\n",
            "        *   `ConversationBufferWindowMemory`: Stores a limited number of recent turns in the conversation.\n",
            "\n",
            "5.  **Chains:**\n",
            "    *   Sequences of calls to LLMs or other utilities.\n",
            "    *   Agents often use chains to perform specific tasks, such as formatting the output of a tool or summarizing a document.\n",
            "\n",
            "6. **Planning and Execution:**\n",
            "   * Some advanced multi-agent systems involve a planning stage where agents collaborate to create a plan of action before executing it. This can improve coordination and efficiency.\n",
            "\n",
            "**Basic Workflow of an Agent:**\n",
            "\n",
            "1.  **Receive Input:** The agent receives a user query or task.\n",
            "2.  **Observe:** The agent analyzes the input and its current state (including memory).\n",
            "3.  **Think:** The LLM processes the information and decides on the next action.\n",
            "4.  **Act:** The agent selects a tool and provides input to the tool.\n",
            "5.  **Observe (Result):** The agent receives the output from the tool.\n",
            "6.  **Repeat:** Steps 3-5 are repeated until the agent has achieved its goal or reached a stopping condition.\n",
            "7.  **Respond:** The agent provides a final answer or result to the user.\n",
            "\n",
            "**Example:  A Simple Agent Using a Search Engine**\n",
            "\n",
            "```python\n",
            "from langchain.agents import initialize_agent, Tool\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.utilities import SerpAPIWrapper\n",
            "import os\n",
            "\n",
            "# Set your OpenAI API key (replace with your actual key)\n",
            "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"  # Replace with your API key\n",
            "# Set your SerpAPI key (replace with your actual key)\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
            "\n",
            "# Initialize the LLM (e.g., OpenAI's GPT-3.5)\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Define the tools the agent can use (in this case, a search engine)\n",
            "search = SerpAPIWrapper()\n",
            "tools = [\n",
            "    Tool(\n",
            "        name=\"Search\",\n",
            "        func=search.run,\n",
            "        description=\"useful for when you need to answer questions about current events. You should ask very specific questions\",\n",
            "    )\n",
            "]\n",
            "\n",
            "# Initialize the agent\n",
            "agent = initialize_agent(\n",
            "    tools,\n",
            "    llm,\n",
            "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
            "    verbose=True,\n",
            ")\n",
            "\n",
            "# Run the agent\n",
            "query = \"What is the current temperature in London?\"\n",
            "response = agent.run(query)\n",
            "print(response)\n",
            "```\n",
            "\n",
            "**Explanation of the Code:**\n",
            "\n",
            "1.  **Import Libraries:** Imports necessary modules from Langchain.\n",
            "2.  **Set API Keys:**  Sets the OpenAI and SerpAPI API keys as environment variables.  **Important:**  Never hardcode API keys directly in your code. Use environment variables or a secrets management system.\n",
            "3.  **Initialize LLM:** Creates an instance of the OpenAI language model.  `temperature=0` makes the LLM more deterministic.\n",
            "4.  **Define Tools:** Creates a `Tool` object that represents the search engine.  The `description` is crucial; it tells the agent when to use this tool.\n",
            "5.  **Initialize Agent:**\n",
            "    *   `initialize_agent` creates the agent.\n",
            "    *   `tools`:  The list of tools the agent can use.\n",
            "    *   `llm`:  The language model.\n",
            "    *   `agent`: Specifies the type of agent (e.g., `AgentType.ZERO_SHOT_REACT_DESCRIPTION`).  This agent type uses only the tool descriptions to decide which tool to use.\n",
            "    *   `verbose=True`:  Prints the agent's thought process to the console, which is helpful for debugging.\n",
            "6.  **Run Agent:**\n",
            "    *   `agent.run(query)` executes the agent with the given query.\n",
            "    *   The agent will use the LLM to decide whether to use the search tool.  If it decides to use the search tool, it will pass the query to the search engine and then use the LLM to generate a final answer based on the search results.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **API Keys:**  You'll need API keys for the LLM (e.g., OpenAI) and any tools you use (e.g., SerpAPI, Google Search).\n",
            "*   **Prompt Engineering:**  The prompt you provide to the agent significantly impacts its behavior.  Craft clear and specific prompts to guide the agent's reasoning.\n",
            "*   **Tool Descriptions:** The descriptions of your tools are critical for the agent to understand when and how to use them.\n",
            "*   **Error Handling:** Implement robust error handling to catch exceptions and prevent the agent from crashing.\n",
            "*   **Security:** Be mindful of security risks when using external tools and APIs.  Sanitize inputs and validate outputs to prevent vulnerabilities.\n",
            "*   **Cost:** Using LLMs can be expensive.  Monitor your API usage and set limits to avoid unexpected charges.\n",
            "\n",
            "**More Advanced Multi-Agent Scenarios**\n",
            "\n",
            "*   **Chatbots with Multiple Personalities:** Create a chatbot with different agents representing different personalities or areas of expertise.\n",
            "*   **Task Decomposition and Coordination:**  Use multiple agents to break down a complex task into smaller sub-tasks and coordinate their execution.\n",
            "*   **Negotiation and Collaboration:**  Develop agents that can negotiate with each other to reach agreements or solve problems collaboratively.\n",
            "*   **Planning and Execution:**  Agents can work together to create a plan of action before executing it.\n",
            "*   **Simulations:**  Model real-world scenarios with multiple interacting agents.\n",
            "\n",
            "**Example: Two Agents with Different Roles (Conceptual)**\n",
            "\n",
            "Imagine you want to build a system to plan a trip.  You could have:\n",
            "\n",
            "*   **Planner Agent:** Responsible for creating the itinerary, considering user preferences and constraints.  This agent might use tools for searching for flights, hotels, and activities.\n",
            "*   **Booking Agent:** Responsible for making reservations for flights, hotels, and activities.  This agent would need access to booking APIs.\n",
            "\n",
            "The Planner Agent would first create a plan, and then the Booking Agent would execute the plan by making the necessary reservations.\n",
            "\n",
            "**Where to Go Next:**\n",
            "\n",
            "*   **Langchain Documentation:**  The official Langchain documentation is the best resource for learning about multi-agent systems.  Look for the sections on agents, tools, and agent executors.\n",
            "*   **Langchain Examples:**  Explore the Langchain examples repository for practical examples of multi-agent systems.\n",
            "*   **Research Papers:**  Read research papers on multi-agent systems to gain a deeper understanding of the underlying concepts and algorithms.\n",
            "\n",
            "**Important Notes on Langchain Updates:**\n",
            "\n",
            "The Langchain library is under active development, so the API may change. Always refer to the latest documentation for the most up-to-date information.\n",
            "\n",
            "This is a starting point.  Multi-agent systems are a complex and evolving field.  Experiment with different approaches and tools to find what works best for your specific use case. Good luck!\n",
            "User: can you explain your code on which you are built to a leyman\n",
            "Assistant: Okay, let's break down how I work in a way that's easy to understand, without getting too technical.\n",
            "\n",
            "**Imagine I'm a really, really complicated recipe book.**\n",
            "\n",
            "That's the best analogy.  Here's how the recipe book works:\n",
            "\n",
            "*   **The Recipes (My Training Data):** This is the core of everything.  I've read (or been \"trained\" on) a massive collection of text and code. Think of it as millions of cookbooks filled with different recipes for writing, answering questions, translating, summarizing, and much more.  This includes:\n",
            "    *   **Books:** Novels, textbooks, articles, etc.\n",
            "    *   **Websites:** Wikipedia, news sites, blogs, forums, etc.\n",
            "    *   **Code:** Examples of Python, Java, C++, and other programming languages.\n",
            "    *   **Conversations:** Examples of people talking to each other.\n",
            "\n",
            "    The more recipes I have, the better I become at \"cooking\" up the right response.\n",
            "\n",
            "*   **The Index (My Model):** This isn't just a simple list of ingredients. It's a complex map of relationships between words, concepts, and ideas that I learned from the training data.  It's like a super-advanced index that not only tells me *where* a word is used, but also *how* it's used, *what* words usually appear around it, and *what* the overall meaning is.  This is the \"model\" part.  It's a mathematical representation of all the patterns I've learned.\n",
            "\n",
            "*   **The Chef (My Algorithm/Software):** This is the \"brain\" that uses the recipe book (training data) and the index (model) to create something new.  When you ask me a question, the chef does the following:\n",
            "\n",
            "    1.  **Understands the Request:**  The chef carefully reads your question, breaking it down into key words and phrases. It tries to figure out what you're really asking.\n",
            "\n",
            "    2.  **Looks Up Relevant Recipes:** Based on the keywords in your question, the chef searches the index (model) to find the most relevant \"recipes\" (patterns and information) from the training data.\n",
            "\n",
            "    3.  **Combines and Adapts:** The chef doesn't just copy and paste a recipe. Instead, it combines the information from multiple recipes, adapts them to the specific question, and generates a new, original answer.\n",
            "\n",
            "    4.  **Outputs the Result:** The chef presents the finished \"dish\" – my answer – to you.\n",
            "\n",
            "**Key Concepts Simplified:**\n",
            "\n",
            "*   **Training:**  The process of feeding me lots of data (the recipes) so I can learn the patterns and relationships between words and concepts.\n",
            "*   **Model:** The complex index that stores all the information I've learned during training.  It's like a highly organized and interconnected map of knowledge.\n",
            "*   **Algorithm:** The software that uses the model to understand your questions and generate answers.  It's the chef that does the actual \"cooking.\"\n",
            "*   **Neural Networks:**  This is the type of architecture used to build my model. Think of it as a network of interconnected nodes (like neurons in a brain) that process information.  The connections between these nodes are strengthened or weakened during training, allowing me to learn complex patterns.\n",
            "\n",
            "**Important Points:**\n",
            "\n",
            "*   **I Don't \"Think\" Like a Human:** I don't have consciousness, emotions, or personal beliefs. I'm a machine that's been trained to recognize patterns in data and generate text based on those patterns.\n",
            "*   **I Can Make Mistakes:** Because I'm trained on data created by humans, I can sometimes reflect biases present in that data. I can also get things wrong if the training data is incomplete or inaccurate.\n",
            "*   **I'm Constantly Evolving:** My creators are constantly working to improve my training data, model, and algorithms. This means I'm getting better and more capable over time.\n",
            "\n",
            "**In a nutshell, I'm like a super-advanced recipe book that can understand your requests, find the right ingredients, and cook up a new and original answer just for you.**\n",
            "\n",
            "Is that easier to understand? Let me know if you have any other questions.\n",
            "User: quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}